{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 1 Springboard\n",
    "## Predictive Maintenance Utilizing CRISP-DM\n",
    "\n",
    "### Background\n",
    "\n",
    "Now that the hype surrounding Data Science has slightly diminished, we can affirm that this is not a drill but rather an exhilarating reality. Governments, large organizations, and start-ups alike have already seen and understood the value this discipline brings to the table and are fervently competing for talent and dominance in this space. As of 2019, we are finding ourselves at a new precipice in entering the Fourth Industrial Revolution: The Real-Time Enterprise! Just like the intersections of the past: First in 1784, Water and steam; Second in 1870, First conveyor belt; and the Third in 1969, Electronics and information technology, this pause will bring with it both challenges and new opportunities. Data stewards in the discipline have realized associative costs for data storage has expounded this issue while driving talent gaps. As their respective organizations continue to ingest voluminous amounts of data, they must become more tactical with the data they are creating and using to sustain or improve Return on Investment (ROI). \n",
    "More so, the approach of advancing insight through analyses of mountains of data must clear and consistent so that results are both reproducible and can be made autonomous. One method in doing this is by utilizing the Cross-Industry Standard Process for Data Mining (CRISP-DM). This logical method enables data stewards and stakeholders to clearly understand what, when, where, why, and how they are mining data through six easy to follow phases:\n",
    "\n",
    "![crisp-dm-phases](Data/crisp-dm-phases.png)\n",
    "\n",
    ">\t1. Business understanding\n",
    ">\t2. Data understanding\n",
    ">\t3. Data prep\n",
    ">\t4. Modeling & Application Development\n",
    ">\t5. Evaluation\n",
    ">\t6. Model Deployment. \n",
    "\n",
    "As talent fluctuations occur in organizations, reproducibility is facilitated by accurately recording steps taken in each phase. The structure will not only assist firms in creating better business outcomes; it will also enable consistent and reproducible workflows through transitioning data science practitioners.\n",
    "\n",
    "### Phase I. Business Understanding.\n",
    "\n",
    "The following project will demonstrate how to utilize CRISP-DM from a practical standpoint; the next analog will use it on simulated manufacturing data predicting maintenance failures for a theoretical client's manufacturing operation. Predictive maintenance is an area that has a clear use-case for data-mining and analytics and primarily due to the breakthroughs of applied machine learning. With the continuous advancements in the Real-time enterprise fueled through the: Internet of Things (IoT), Telemetry, Low-Cost Digital Storage, and increasing Computing Power amongst others, the capabilities of transforming voluminous data into insight in this space does not appear to be slowing. The growth in Artificial Intelligence (AI) amongst increasing levels of Automation seen in manufacturing allows firms to be more resilient in connecting fixed-assets while improving productivity through data-driven decisions and insights not possible before. As the use of automation continues to augment and takeover manufacturing, the reduced response time required in dealing with maintenance issues will outpace the speed at which humans can intervene, requiring sophisticated and automated optimization decisions, especially about maintenance schedules. Executives and managers in both the public and private sectors to cope with this transition must speed up training initiatives to groom new tech talent to utilize tools to assist them in managing this transition through a structured method, or else the cost of doing business will outweigh the profits of its outputs.\n",
    "\n",
    "**Who might care?** Maintenance Managers, Operations Managers, Capital Expenditure planners, and manufacturing companies such as Exon Mobile Corporation, General Motors, Ford Motors, Apple, Boeing, and the Department of Defense, etc. can use such a model to predict the likelihood of equipment failures to allocate resources better. They can then proactively inform their maintainers and or customers well in advance of potential disruptions in their respective operations. Understanding the probability of material failures will help sustain customer service or level of service efforts. From the customer's point of view, it would be very convenient in knowing if a supply, production, or any other disruption may occur so that they can in turn, proactively mitigate risk. On the manufacturer's hand, such a predictive model would enhance the product base and performance of the organization's operations. Moreover, there is a possibility of developing an app or other front-end communication effort in which customers and or internal users can consult with to understand the likelihood of issues well in advance.\n",
    "\n",
    "> **Cost and benefits**\n",
    "\n",
    "> • Every maintenance hour reduced in human labor will save the company and average of 75.69 which includes fringe benefits. The company's current budget for maintenance includes a staff of 70 maintainers overseeing 10 machines (700 machines total) each week working 40 hours a week at an operating expense of 211,932.00 a week. Due to high attrition the firm would like to augment the operations team with a data scientist at a cost of 125.00 an hour fully burdened to help optimize and allocate maintenance labor hours. The operations team has refitted each of the 700 machines with telemetry sensors.\n",
    "\n",
    "__Set objectives:__\n",
    "\n",
    "> 1. In this case the client has furnished publicly available sensor data on 1,900 machines which was collected over a span of 4 years. The machines recorded observations over time that include the following sample features: Device ID; Time Stamp; Warning Flags; Error/Issues; and Target Variable: Fail/No Fail. Based on the data provided the client wanted us to predict what mix of sensor readings would trigger a response: Normal, Fail, and Recovering.\n",
    "\n",
    "> 2. Are there any leading conditions that exist with the sensors before a fail scenario occurs.\n",
    "\n",
    "\n",
    "__Contraints, limitations, and assumptions__\n",
    "\n",
    "> • _Constraint 1:_ The success of the model will depend on how good the prediction of material failure is based on historical observations in the curated data. For instance after analyzing several observations of data after X period of months pertaining to consistent machine utilization hours the model's predictions will be based on future and consistent occurences similar to the recorded and analyzed observations in the past. \n",
    "\n",
    "> • _Limitations 1:_ In order to predict the likelihood of a future material failure for a machine after X period of running hours, we would need to know how many hours the machine is expected to run in the future: which we do not know \"accurately\" today. \n",
    "\n",
    "> • _Assumptions 1:_ The classification model will be able to communicated probablities at various run levels to address this limitation. \n",
    "\n",
    "__Risk and contigencies__\n",
    "                \n",
    "> • _Risk 1 Scheduling: What if the project takes longer than anticipated?_ Consultant will monitor work performance and proactively partner with stakeholder to seek additional time or alternative recommendations from stakeholder.\n",
    "\n",
    "> • _Risk 2 Financial: What if the project sponsor encounters budgetary problems?_ Consultant will monitor work performance and proactively partner with stakeholder to seek alternative recommendations from stakeholder pertaining to trade-space opportunities.\n",
    "\n",
    "> • _Risk 3 Data: What if the data are of poor quality or coverage?_ Consultant will monitor work performance and proactively partner with stakeholder to seek alternative recommendations from stakeholder pertaining to trade-space opportunities concerning scope change in acquiring additional data applicable to this project.\n",
    "\n",
    "> • _Risk 4 Results: What if the initial results are less dramatic than expected?_ Consultant will monitor work performance and project phases in order to proactively partner with stakeholder to evaluate go or no-go empasses to ensure all partners are in agreement before moving on to the next phase of CRISP-DM. Each phase will be signed off by stakeholder to ensure all concerned parties are in agreement of progress.\n",
    "\n",
    "\n",
    "__Terminology__\n",
    "                \n",
    "> • All definitions involved in this project could be found [here](Data\\Data_Mining_Glossary.csv).\n",
    "\n",
    "        \n",
    "**Business/Data mining goals** describe the intended outputs of the project that enable the achievement of the business objectives.\n",
    "        a. Business success criteria\n",
    "        b. Data mining success criteria\n",
    "\n",
    "**Project plan**  Describe the intended plan for achieving the data mining goals and thereby achieving the business goals. Your plan should specify the steps to be performed during the rest of the project, including the initial selection of tools and techniques.\n",
    "        a. Project plan:\n",
    "\n",
    "This overview will be structured using the CRISP-DM framework: \n",
    "\n",
    "**Phase I. Business Understanding:**\n",
    "_Current situation assessment_\n",
    "> a.    Set objectives\n",
    "> b.    Inventory of resources\n",
    "> c.    Requirements, assumptions and constraints\n",
    "> d.    Risks and contingencies\n",
    "> e.    Terminology\n",
    "> f.    Cost and benefits\n",
    "        \n",
    "**Business/Data mining goals**\n",
    "> a.    Business success criteria\n",
    "> b.    Data mining success criteria\n",
    "        \n",
    "_Project plan_\n",
    "> a.    Project plan\n",
    "> b.    Initial assessment of tools and techniques  \n",
    "\n",
    "*Review and approval point*  \n",
    "\n",
    "\n",
    "**Phase II. Data Understanding:**\n",
    "_Data collection_\n",
    "> a.    Initial data collection report\n",
    "\n",
    "_Data Description_\n",
    "> a.    Data description report\n",
    "\n",
    "_Data Exploration_\n",
    "> a.    Data exploration report\n",
    "\n",
    "_Data Quality_\n",
    "> a.    Data quality report\n",
    "\n",
    "_Review and approval point_\n",
    "\n",
    "\n",
    "**Phase III. Data Preparation:** \n",
    "> a.    Rational for inclusion/exclusion\n",
    "> b.    Data cleaning report\n",
    "> c.    Derived attributes\n",
    "> d.    Generated records\n",
    "> e.    Merged data\n",
    "> f.    Aggregations\n",
    "\n",
    "_Review and approval point_\n",
    "\n",
    "\n",
    "**Phase IV. Data Modeling & Application Development:**\n",
    "_Technique_\n",
    "> a.    Modeling technique\n",
    "> b.    Modeling assumptions\n",
    "\n",
    "_Test design_\n",
    "> a.    Test design\n",
    "\n",
    "_Build model_\n",
    "> a.    Parameter settings\n",
    "> b.    Models\n",
    "        \n",
    "_Model assessment_\n",
    "> a.    Model assessment\n",
    "> b.    Revised parameter settings\n",
    "\n",
    "_Review and approval point_\n",
    "\n",
    "\n",
    "**Phase V. Model Evaluation:**\n",
    "Evaluation\n",
    "> a.    Assessment of data mining results\n",
    "> b.    Approved model results\n",
    "\n",
    "_Reviewal process_\n",
    "> a.    Review of process\n",
    "\n",
    "_Next steps_\n",
    "> a.    List of course of actions (COAs)\n",
    "> b.    Final Decision/Selection of COA\n",
    "\n",
    "_Review and approval point_\n",
    "\n",
    "**Phase VI. Model Deployment and Communication:**\n",
    "_Plan deployment_\n",
    "> a.    Plan deployment\n",
    "\n",
    "_Plan monitoring and maintenance_\n",
    "> a.    Montoring and maintenance plan\n",
    "        \n",
    "_Produce final report_\n",
    "> a.    Final report\n",
    "> b.    Final presentation\n",
    "\n",
    "_Review/Archiving of process_\n",
    "> a.    Experience documentation\n",
    "\n",
    "\n",
    "***\n",
    "__Initial assessment of resources and tools:__\n",
    "\n",
    "> 1.  __Resourse:__ Data Scientist, Alfred Hull\n",
    "> 2.  __Resourse:__ The data in this overview is simulated at about 37 Megabytes (MB) to demonstrate how machines performed over a time frame of five months: 31-March-2018 to 31-August-2018. The data contains a 222k by 55 matrix: two hundred twenty two observations by 55 fields.\n",
    "> 3.  __Tool:__ The computing resources used for this example were the MS Surface Book 2 for Business - 15\" Display /256 GB / Intel Core i7. High-speed Intel processors, (quad-core available), NVIDIA GeForce GTX graphics, 17 hours of battery life, and running Windows 10 Pro.\n",
    "> 4.  __Tool:__ The software used for these examples was Windows 10, Anaconda, IPython, and Jupyter Lab\n",
    "\n",
    "\n",
    "__Reference:__\n",
    "\n",
    "> 1.  Github Repository: https://github.com/ahull002/wrangling_csv.git\n",
    "> 2.  folium documentation: https://python-visualization.github.io/folium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
